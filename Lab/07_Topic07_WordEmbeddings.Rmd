---
title: 'Topic 7: Word Embeddings'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This week's Rmd file here: <https://github.com/MaRo406/EDS_231-text-sentiment/blob/main/topic_7.Rmd>

```{r packages, include = FALSE}
library(here)
library(tidytext)
library(tidyverse)
library(widyr)
library(irlba) #singluar value decomposition
library(broom) # creating search_synonym function
library(textdata)
library(ggplot2)
library(dplyr)

#https://semantle.com/
```

Today we are using climbing incident data from this repo: <https://github.com/ecaroom/climbing-accidents>. Some analysis (in Excel) on the data was written up into a Rock and Ice magazine article.

But I've constructed our data set (link below) by pulling a few key variables including the full text of each incident report.

```{r data,}
incidents_df <- read_csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/825b159b6da4c7040ce8295b9eae2fbbe9991ffd/dat/climbing_report_text.csv")
```

First, let's calculate the unigram probabilities, how often we see each word in this corpus.

```{r unigrams}
unigram_probs <- incidents_df %>%
    unnest_tokens(word, Text) %>%
    anti_join(stop_words, by = 'word') %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n)) 

unigram_probs 
```

Next, we need to know how often we find each word near each other word -- the skipgram probabilities. This is where we use the sliding window.

Windowing process - look at first few words of a sentence. Shows these occurred together to capture semantic relatedness

```{r}
# semantic relatedness set to a window size of 5

skipgrams <- incidents_df %>%
    unnest_tokens(ngram, Text, token = "ngrams", n = 5) %>%
    mutate(ngramID = row_number()) %>% 
    tidyr::unite(skipgramID, ID, ngramID) %>%
    unnest_tokens(word, ngram) %>%
    anti_join(stop_words, by = 'word')

skipgrams
# provides title of report, word column is window (3 m words)
```

```{r}

skipgram_probs <- skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n)) # normalize frequency, measurement of how words occur together
```

Having all the skipgram windows lets us calculate how often words together occur within a window, relative to their total occurrences in the data. We do this using the point-wise mutual information (PMI). It's the logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone. PMI tells us which words occur together more often than expected based on how often they occurred on their own.


```{r norm-prob}
# normalize probabilities

#calculate probabilities that a word co-occurs in a window and how frequent that word occurs (pointwise mutual information), normalize it for occurrence frequency

normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

#Which words are most associated with "rope"?   
normalized_prob %>% 
    filter(word1 == "rope") %>%
    arrange(-p_together)
```

Now we convert to a matrix so we can use matrix factorization and reduce the dimensionality of the data.

```{r pmi}
# decomposes a single matrix keep max info on how words are related
pmi_matrix <- normalized_prob %>%
    mutate(pmi = log10(p_together)) %>%
    cast_sparse(word1, word2, pmi)    
 
#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

#run SVD using irlba() which is good for sparse matrices 
pmi_svd <- irlba(pmi_matrix, 100, maxit = 500) #Reducing to 100 dimensions

#next we output the word vectors:
word_vectors <- pmi_svd$u
rownames(word_vectors) <- rownames(pmi_matrix) # dimensions that words co-occur
```

```{r syn-function}
# function to create similarity score
search_synonyms <- function(word_vectors, selected_vector) {
dat <- word_vectors %*% selected_vector
    
similarities <- dat %>%
        tibble(token = rownames(dat), similarity = dat[,1])

similarities %>%
       arrange(-similarity) %>%
        select(c(2,3))
}
```

```{r find-synonyms}
# use function: give  all word vectors (model) and the word " " to calculate similarities
fall <- search_synonyms(word_vectors,word_vectors["fall",])
slip <- search_synonyms(word_vectors,word_vectors["slip",])

snow <- search_synonyms(word_vectors,word_vectors["snow",])
```

```{r plot-synonyms}
# 

slip %>%
    mutate(selected = "slip") %>%
    bind_rows(fall %>%
                  mutate(selected = "fall")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~selected, scales = "free") +
    coord_flip() +
    theme(strip.text=element_text(hjust=0, size=12)) +
    scale_y_continuous(expand = c(0,0)) +
    labs(x = NULL, title = "What word vectors are most similar to slip or fall?")
         
```

```{r word-math}
# vector math - with sufficient data: snow PLUS danger
snow_danger <- word_vectors["snow",] + word_vectors["danger",] \

search_synonyms(word_vectors, snow_danger)

# vector math - with sufficient data: danger MINUS snow, take out snow from associations with "danger"
no_snow_danger <- word_vectors["danger",] - word_vectors["snow",] 

search_synonyms(word_vectors, no_snow_danger)
```

### Assignment

Download a set of pretrained vectors, GloVe, and explore them. 

Grab data here:
<!-- download.file('<https://nlp.stanford.edu/data/glove.6B.zip>',destfile = 'glove.6B.zip')  -->
<!-- unzip('glove.6B.zip')  -->
<!-- Use this file: 'glove.6B.300d.txt' -->

1.  Recreate the analyses in the last three chunks (find-synonyms, plot-synonyms, word-math) with the GloVe embeddings. How are they different from the embeddings created from the climbing accident data? Why do you think they are different?

2.  Run the classic word math equation, "king" - "man" = ?

3.  Think of three new word math equations. They can involve any words you'd like, whatever catches your interest.